{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eb62725c",
   "metadata": {
    "id": "eb62725c"
   },
   "source": [
    "# Tugas 2\n",
    "\n",
    "Prio Budi Laksono\n",
    "\n",
    "210411100177\n",
    "\n",
    "Preprocessing hasil crawling data dari tribunnews.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b47f17be-833a-4b8e-b0ba-f844261c95df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Menambang kategori Otomotif...\n",
      "Mengambil: https://www.tribunnews.com/otomotif?page=1\n",
      "Mengambil: https://www.tribunnews.com/otomotif?page=2\n",
      "Mengambil: https://www.tribunnews.com/otomotif?page=3\n",
      "Menambang kategori Sport...\n",
      "Mengambil: https://www.tribunnews.com/sport?page=1\n",
      "Mengambil: https://www.tribunnews.com/sport?page=2\n",
      "Mengambil: https://www.tribunnews.com/sport?page=3\n",
      "Data berhasil ditambang dan disimpan dalam 'tribunnews_articles.csv'.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "# Fungsi untuk mendapatkan konten dari URL dengan User-Agent\n",
    "def get_soup(url):\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/92.0.4515.107 Safari/537.36'\n",
    "    }\n",
    "    try:\n",
    "        response = requests.get(url, headers=headers)\n",
    "        response.raise_for_status()\n",
    "        return BeautifulSoup(response.content, 'html.parser')\n",
    "    except requests.RequestException as e:\n",
    "        print(f\"Kesalahan saat mengambil {url}: {e}\")\n",
    "        return None\n",
    "\n",
    "# Fungsi untuk mendapatkan detail artikel dari halaman detail\n",
    "# Fungsi untuk mendapatkan detail artikel dari halaman detail dengan penambahan kategori dari variabel 'categories'\n",
    "def get_article_details(detail_url, category_name):\n",
    "    detail_soup = get_soup(detail_url)\n",
    "    if detail_soup:\n",
    "        content = ' '.join([p.text for p in detail_soup.find_all('p')])\n",
    "        date_tag = detail_soup.find('time')\n",
    "        date = date_tag.text.strip() if date_tag else 'Tidak ada tanggal'\n",
    "        title_tag = detail_soup.find('h1')\n",
    "        title = title_tag.text.strip() if title_tag else 'Tidak ada judul'\n",
    "\n",
    "        # Gunakan kategori yang diambil dari variabel 'categories' alih-alih dari breadcrumb\n",
    "        category = category_name\n",
    "\n",
    "        return {\n",
    "            'judul': title,\n",
    "            'isi_berita': content,\n",
    "            'tanggal': date,\n",
    "            'kategori': category,\n",
    "            'url': detail_url\n",
    "        }\n",
    "    return None\n",
    "\n",
    "\n",
    "# Fungsi untuk mendapatkan artikel dari suatu kategori\n",
    "def get_articles(category_url, category_name, max_articles=100):\n",
    "    articles = []\n",
    "    page = 1\n",
    "\n",
    "    while len(articles) < max_articles:\n",
    "        url = f'{category_url}?page={page}'\n",
    "        print(f\"Mengambil: {url}\")\n",
    "        soup = get_soup(url)\n",
    "\n",
    "        if soup is None:\n",
    "            break\n",
    "\n",
    "        article_list = soup.find_all('h3')\n",
    "\n",
    "        if not article_list:\n",
    "            print(f\"Tidak ada artikel ditemukan di halaman {page}.\")\n",
    "            break\n",
    "\n",
    "        for article in article_list:\n",
    "            if len(articles) >= max_articles:\n",
    "                break\n",
    "\n",
    "            title_tag = article.find('a')\n",
    "            detail_url = title_tag['href'] if title_tag else None\n",
    "\n",
    "            if detail_url:\n",
    "                if not detail_url.startswith('http'):\n",
    "                    detail_url = f'https://www.tribunnews.com{detail_url}'\n",
    "                \n",
    "                # Pemanggilan fungsi dengan kedua argumen\n",
    "                article_details = get_article_details(detail_url, category_name)\n",
    "                \n",
    "                if article_details:\n",
    "                    articles.append(article_details)\n",
    "\n",
    "        page += 1\n",
    "        time.sleep(2)  # Beri jeda agar tidak terlalu cepat melakukan permintaan\n",
    "\n",
    "    return articles\n",
    "\n",
    "\n",
    "# URL Kategori\n",
    "categories = {\n",
    "    'Otomotif': 'https://www.tribunnews.com/otomotif',\n",
    "    'Sport': 'https://www.tribunnews.com/sport'\n",
    "}\n",
    "\n",
    "# Mengumpulkan data dari dua kategori\n",
    "all_articles = []\n",
    "for category_name, category_url in categories.items():\n",
    "    print(f\"Menambang kategori {category_name}...\")\n",
    "    articles = get_articles(category_url, category_name, max_articles=100)\n",
    "    all_articles.extend(articles)\n",
    "\n",
    "# Simpan ke dalam DataFrame\n",
    "df = pd.DataFrame(all_articles)\n",
    "\n",
    "# Simpan ke dalam file CSV\n",
    "df.to_csv('tribunnews_articles.csv', index=False)\n",
    "\n",
    "print(\"Data berhasil ditambang dan disimpan dalam 'tribunnews_articles.csv'.\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91b77f46-6a1a-463d-9638-cb81549179f2",
   "metadata": {},
   "source": [
    "## Preprocessing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "baf3148a-28cc-47b6-ae9b-bfed5b3cdc0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing selesai dan hasil disimpan dalam 'tribunnews_articles_preprocessed.csv'.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "# Unduh stopwords jika belum diunduh\n",
    "try:\n",
    "    nltk.download('stopwords', quiet=True)\n",
    "except Exception as e:\n",
    "    print(f\"Error downloading stopwords: {e}\")\n",
    "\n",
    "# Fungsi untuk membersihkan dan melakukan stemming pada teks\n",
    "def preprocess_text(text):\n",
    "    # Hapus HTML\n",
    "    text = re.sub(r'<[^>]+>', '', text)\n",
    "    # Hapus angka dan tanda baca\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    # Ubah menjadi huruf kecil\n",
    "    text = text.lower()\n",
    "    # Tokenisasi\n",
    "    tokens = text.split()\n",
    "    # Hapus stopwords\n",
    "    stop_words = set(stopwords.words('indonesian'))\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "    # Stemming\n",
    "    ps = PorterStemmer()\n",
    "    stemmed_tokens = [ps.stem(word) for word in tokens]\n",
    "    return ' '.join(stemmed_tokens)\n",
    "\n",
    "# Load data dari CSV\n",
    "try:\n",
    "    df = pd.read_csv(r'C:\\Users\\pblak\\OneDrive\\Desktop\\Tugas Kuliah\\Semester 7\\Pencarian dan Penambangan WEB\\tribunnews_articles.csv')\n",
    "except FileNotFoundError:\n",
    "    print(\"File tidak ditemukan. Pastikan path file CSV benar.\")\n",
    "    raise\n",
    "\n",
    "# Terapkan preprocessing ke setiap konten artikel\n",
    "df['isi_berita_clean'] = df['isi_berita'].apply(preprocess_text)\n",
    "\n",
    "# Simpan ke CSV hasil preprocessing\n",
    "df.to_csv(r'C:\\Users\\pblak\\OneDrive\\Desktop\\Tugas Kuliah\\Semester 7\\Pencarian dan Penambangan WEB\\tribunnews_articles_preprocessed.csv', index=False)\n",
    "\n",
    "print(\"Preprocessing selesai dan hasil disimpan dalam 'tribunnews_articles_preprocessed.csv'.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96a43665-7649-44e7-8d83-096121a22db0",
   "metadata": {},
   "source": [
    "## Membangun model logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "590520a1-7c8a-45a8-88af-40803dd3cf30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Akurasi model: 0.90\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Membaca data yang telah diproses\n",
    "df = pd.read_csv('tribunnews_articles_preprocessed.csv')\n",
    "\n",
    "# Misalkan kita ingin mengklasifikasikan kategori berdasarkan 'isi_berita_clean'\n",
    "X = df['isi_berita_clean']\n",
    "y = df['kategori']\n",
    "\n",
    "# Membagi data menjadi data latih dan data uji\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Membangun pipeline\n",
    "pipeline = Pipeline([\n",
    "    ('vectorizer', CountVectorizer()),\n",
    "    ('classifier', LogisticRegression(max_iter=1000))\n",
    "])\n",
    "\n",
    "# Melatih model\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Mengevaluasi model\n",
    "accuracy = pipeline.score(X_test, y_test)\n",
    "print(f\"Akurasi model: {accuracy:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a505d6e1-b006-4122-a3b7-e23d7ef3149d",
   "metadata": {},
   "source": [
    "## Berikut untuk streamlit lokalhostnya : http://localhost:8502/"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
