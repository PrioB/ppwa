{"cells":[{"cell_type":"markdown","id":"eb62725c","metadata":{"id":"eb62725c"},"source":["# Tugas 2\n","\n","Prio Budi Laksono\n","\n","210411100177\n","\n","Preprocessing hasil crawling data dari jatim.tribunnews.com"]},{"cell_type":"code","execution_count":3,"id":"39820b95","metadata":{"id":"39820b95","outputId":"1e28b3b6-f0a5-4a9d-a5ee-2f0c566a5a73","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1729107595355,"user_tz":-420,"elapsed":753,"user":{"displayName":"21_177 PRIO BUDI LAKSONO","userId":"05194090561344682492"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Menambang kategori Travel...\n","Mengambil: https://jatim.tribunnews.com/travel?page=1\n","Kesalahan saat mengambil https://jatim.tribunnews.com/travel?page=1: 403 Client Error: Forbidden for url: https://jatim.tribunnews.com/travel?page=1\n","Menambang kategori Sport...\n","Mengambil: https://jatim.tribunnews.com/sport?page=1\n","Kesalahan saat mengambil https://jatim.tribunnews.com/sport?page=1: 403 Client Error: Forbidden for url: https://jatim.tribunnews.com/sport?page=1\n","Jumlah artikel yang diambil: 0\n","Peringatan: Tidak ada data yang dikumpulkan, pastikan proses scraping berhasil.\n","Tidak ada data yang tersimpan dalam file CSV karena proses scraping gagal.\n"]}],"source":["import requests\n","from bs4 import BeautifulSoup\n","import pandas as pd\n","import time\n","import os\n","\n","# Fungsi untuk mendapatkan konten dari URL\n","def get_soup(url):\n","    try:\n","        response = requests.get(url)\n","        response.raise_for_status()\n","        return BeautifulSoup(response.content, 'html.parser')\n","    except requests.RequestException as e:\n","        print(f\"Kesalahan saat mengambil {url}: {e}\")\n","        return None\n","\n","# Fungsi untuk mendapatkan detail artikel dari halaman detail\n","def get_article_details(detail_url):\n","    detail_soup = get_soup(detail_url)\n","    if detail_soup:\n","        # Ambil isi berita\n","        content = ' '.join([p.text for p in detail_soup.find_all('p')])\n","\n","        # Ambil tanggal publikasi\n","        date_tag = detail_soup.find('time')\n","        date = date_tag.text.strip() if date_tag else 'Tidak ada tanggal'\n","\n","        # Ambil judul berita\n","        title_tag = detail_soup.find('h1')\n","        title = title_tag.text.strip() if title_tag else 'Tidak ada judul'\n","\n","        # Ambil kategori dari breadcrumb\n","        breadcrumb = detail_soup.find('ul', {'class': 'breadcrumb'})\n","        category = breadcrumb.find_all('li')[-1].find('span').text.strip() if breadcrumb else 'Tidak ada kategori'\n","\n","        return {\n","            'judul': title,\n","            'isi_berita': content,\n","            'tanggal': date,\n","            'kategori': category,\n","            'url': detail_url\n","        }\n","    return None\n","\n","# Fungsi untuk mendapatkan artikel dari suatu kategori\n","def get_articles(category_url, category_name, max_articles=100):\n","    articles = []\n","    page = 1\n","\n","    while len(articles) < max_articles:\n","        url = f'{category_url}?page={page}'  # Periksa pola yang benar untuk paginasi\n","        print(f\"Mengambil: {url}\")  # Output debug\n","        soup = get_soup(url)\n","\n","        if soup is None:\n","            break\n","\n","        # Cari artikel di halaman\n","        article_list = soup.find_all('h3')  # Berdasarkan tata letak yang diamati\n","\n","        if not article_list:\n","            print(f\"Tidak ada artikel ditemukan di halaman {page}.\")\n","            break\n","\n","        for article in article_list:\n","            if len(articles) >= max_articles:\n","                break\n","\n","            # Ambil URL detail artikel\n","            title_tag = article.find('a')\n","            detail_url = title_tag['href'] if title_tag else None\n","\n","            if detail_url:\n","                if not detail_url.startswith('http'):\n","                    detail_url = f'https://jatim.tribunnews.com{detail_url}'\n","                # Buka halaman detail artikel\n","                article_details = get_article_details(detail_url)\n","                if article_details:\n","                    # Tambahkan ke daftar artikel\n","                    articles.append(article_details)\n","\n","        page += 1\n","        time.sleep(2)  # Beri jeda agar tidak terlalu cepat melakukan permintaan\n","\n","    return articles\n","\n","# URL Kategori\n","categories = {\n","    'Travel': 'https://jatim.tribunnews.com/travel',\n","    'Sport': 'https://jatim.tribunnews.com/sport'\n","}\n","\n","# Mengumpulkan semua data\n","all_articles = []\n","for category_name, category_url in categories.items():\n","    print(f\"Menambang kategori {category_name}...\")\n","    articles = get_articles(category_url, category_name, max_articles=100)\n","    all_articles.extend(articles)\n","\n","# Simpan ke dalam DataFrame\n","df = pd.DataFrame(all_articles)\n","\n","# Debug: Memeriksa apakah data berhasil diambil\n","print(f\"Jumlah artikel yang diambil: {len(df)}\")\n","if len(df) == 0:\n","    print(\"Peringatan: Tidak ada data yang dikumpulkan, pastikan proses scraping berhasil.\")\n","\n","# Simpan ke dalam file CSV hanya jika ada data\n","if not df.empty:\n","    file_path = 'tribunnews_articles.csv'\n","    df.to_csv(file_path, index=False)\n","\n","    # Debug: Memastikan file CSV tersimpan dengan benar\n","    if os.path.exists(file_path):\n","        print(f\"File CSV tersimpan: {file_path}\")\n","    else:\n","        print(\"Kesalahan: File CSV tidak berhasil disimpan.\")\n","\n","    # Membaca kembali file CSV dan menampilkan data\n","    df_loaded = pd.read_csv(file_path)\n","    print(\"Data yang diambil dari file CSV:\")\n","    print(df_loaded.head(10))  # Tampilkan 10 data pertama\n","else:\n","    print(\"Tidak ada data yang tersimpan dalam file CSV karena proses scraping gagal.\")\n"]},{"cell_type":"code","execution_count":2,"id":"60a6d13a","metadata":{"id":"60a6d13a","colab":{"base_uri":"https://localhost:8080/","height":339},"executionInfo":{"status":"error","timestamp":1729107306317,"user_tz":-420,"elapsed":6,"user":{"displayName":"21_177 PRIO BUDI LAKSONO","userId":"05194090561344682492"}},"outputId":"f0951448-58e3-4d97-c7b0-a97a8ae46742"},"outputs":[{"output_type":"error","ename":"EmptyDataError","evalue":"No columns to parse from file","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mEmptyDataError\u001b[0m                            Traceback (most recent call last)","\u001b[0;32m<ipython-input-2-b3919f82cdfb>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"tribunnews_articles.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1619\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1620\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1622\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1896\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1897\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1898\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mmapping\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1899\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1900\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/c_parser_wrapper.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m     91\u001b[0m             \u001b[0;31m# Fail here loudly instead of in cython after reading\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m             \u001b[0mimport_optional_dependency\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"pyarrow\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32mparsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n","\u001b[0;31mEmptyDataError\u001b[0m: No columns to parse from file"]}],"source":["df=pd.read_csv(\"tribunnews_articles.csv\")\n","df.head(1000)"]},{"cell_type":"code","execution_count":4,"id":"1bed4291","metadata":{"scrolled":true,"id":"1bed4291","colab":{"base_uri":"https://localhost:8080/","height":391},"executionInfo":{"status":"error","timestamp":1729107604700,"user_tz":-420,"elapsed":6,"user":{"displayName":"21_177 PRIO BUDI LAKSONO","userId":"05194090561344682492"}},"outputId":"67517569-3b44-40fd-fdd5-9ccf3119cbf6"},"outputs":[{"output_type":"error","ename":"EmptyDataError","evalue":"No columns to parse from file","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mEmptyDataError\u001b[0m                            Traceback (most recent call last)","\u001b[0;32m<ipython-input-4-cc45567666d7>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"tribunnews_articles.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1619\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1620\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1622\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1896\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1897\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1898\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mmapping\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1899\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1900\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/c_parser_wrapper.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m     91\u001b[0m             \u001b[0;31m# Fail here loudly instead of in cython after reading\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m             \u001b[0mimport_optional_dependency\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"pyarrow\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32mparsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n","\u001b[0;31mEmptyDataError\u001b[0m: No columns to parse from file"]}],"source":["import pandas as pd\n","\n","df = pd.read_csv(\"tribunnews_articles.csv\")\n","\n","df.head()"]},{"cell_type":"markdown","id":"89ea7e4d","metadata":{"id":"89ea7e4d"},"source":["## **Preprocessing**\n","\n","Preprocessing adalah proses membersihkan dan mempersiapkan data mentah agar siap digunakan oleh model machine learning. Ini meliputi penanganan data yang hilang, normalisasi, mengubah data kategori menjadi angka, dan membersihkan teks. Tujuannya agar data lebih mudah dipahami dan diolah oleh model untuk hasil yang lebih akurat, Berikut adalah beberapa langkah umum dalam pre-processing teks:"]},{"cell_type":"markdown","id":"92b900c6","metadata":{"id":"92b900c6"},"source":["### Cleansing\n","\n","Proses cleansing data adalah tahap pembersihan teks dari elemen-elemen yang tidak relevan terhadap hasil klasifikasi sentimen. Beberapa komponen yang tidak berpengaruh terhadap sentimen, seperti URL, tag HTML, emoji, simbol, angka, dan tanda baca (~!@#$%^&*{}<>:|), dihapus dari dokumen ulasan. Elemen-elemen tersebut dihilangkan untuk mengurangi kebisingan (noise) dalam data. Setelah dihapus, elemen ini digantikan dengan spasi agar struktur kalimat tetap terjaga. Dengan demikian, data menjadi lebih fokus pada kata-kata yang relevan untuk menentukan sentimen, sehingga membantu meningkatkan akurasi model prediksi sentimen."]},{"cell_type":"code","execution_count":null,"id":"e8ef3790","metadata":{"id":"e8ef3790","executionInfo":{"status":"aborted","timestamp":1729107307707,"user_tz":-420,"elapsed":1394,"user":{"displayName":"21_177 PRIO BUDI LAKSONO","userId":"05194090561344682492"}}},"outputs":[],"source":["import re\n","import pandas as pd\n","import nltk\n","import string\n","\n","def remove_url(text):\n","    #Fungsi untuk menghapus URL dari teks.\n","    url = re.compile(r'https?://\\S+|www\\.\\S+')\n","    return url.sub(r'', text)\n","\n","def remove_html(text):\n","    #Fungsi untuk menghapus tag HTML dari teks.\n","    html = re.compile(r'<.*?>')\n","    return html.sub(r'', text)\n","\n","def remove_emoji(text):\n","\n","    #Fungsi untuk menghapus emoji dari teks.\n","    emoji_pattern = re.compile(\"[\"\n","        u\"\\U0001F600-\\U0001F64F\"  # emotikon wajah\n","        u\"\\U0001F300-\\U0001F5FF\"  # simbol & gambar\n","        u\"\\U0001F680-\\U0001F6FF\"  # transportasi & simbol\n","        u\"\\U0001F1E0-\\U0001F1FF\"  # bendera negara\n","        \"]+\", flags=re.UNICODE)\n","    return emoji_pattern.sub(r'', text)\n","\n","def remove_numbers(text):\n","    #Fungsi untuk menghapus angka dari teks.\n","    return re.sub(r'\\d+', '', text)\n","\n","def remove_symbols(text):\n","    #Fungsi untuk menghapus simbol dan karakter khusus dari teks.\n","    return re.sub(r'[^a-zA-Z0-9\\s]', '', text)\n","\n","# Asumsikan df adalah DataFrame yang berisi data CNN (judul, berita, tanggal, kategori)\n","# Contoh: df = pd.read_csv('berita-cnn.csv')\n","\n","# Terapkan fungsi cleansing untuk kolom 'berita'\n","df['berita_clean'] = df['isi_berita'].apply(remove_url)\n","df['berita_clean'] = df['berita_clean'].apply(remove_html)\n","df['berita_clean'] = df['berita_clean'].apply(remove_emoji)\n","df['berita_clean'] = df['berita_clean'].apply(remove_symbols)\n","df['berita_clean'] = df['berita_clean'].apply(remove_numbers)\n","\n","\n","\n","# Tampilkan beberapa baris dari hasil yang sudah dibersihkan\n","df.head(5)\n"]},{"cell_type":"markdown","id":"6de084e6","metadata":{"id":"6de084e6"},"source":["### CASE FOLDING\n","\n","Pada tahap case folding, semua huruf kapital dalam dokumen ulasan diubah menjadi huruf kecil, atau disebut lowercase. Tujuan dari langkah ini adalah untuk menghilangkan redundansi data yang hanya disebabkan oleh perbedaan kapitalisasi. Misalnya, kata \"Ekonomi\" dan \"ekonomi\" secara teknis sama dalam analisis teks, namun tanpa case folding, komputer akan menganggapnya berbeda. Dengan mengonversi seluruh teks menjadi huruf kecil, semua variasi penulisan diseragamkan, sehingga mencegah duplikasi penghitungan atau kesalahan dalam interpretasi data."]},{"cell_type":"code","execution_count":null,"id":"43478f22","metadata":{"id":"43478f22","executionInfo":{"status":"aborted","timestamp":1729107307708,"user_tz":-420,"elapsed":5,"user":{"displayName":"21_177 PRIO BUDI LAKSONO","userId":"05194090561344682492"}}},"outputs":[],"source":["def case_folding(text):\n","    if isinstance(text, str):\n","      lowercase_text = text.lower()\n","      return lowercase_text\n","    else :\n","      return text\n","\n","df ['case_folding'] = df['berita_clean'].apply(case_folding)\n","\n","df.head(5)"]},{"cell_type":"markdown","id":"834ca053","metadata":{"id":"834ca053"},"source":["### TOKENIZATION\n","\n","Tokenization adalah tahap di mana setiap kata dalam sebuah dokumen dipecah menjadi unit-unit kata yang lebih kecil, atau disebut token. Proses ini memisahkan kata-kata berdasarkan spasi, sehingga setiap kata yang terpisah oleh spasi dianggap sebagai token tersendiri. Sebagai contoh, kalimat \"Upaya agar ekonomi stabil\" akan diuraikan menjadi token [\"Upaya\", \"agar\", \"ekonomi\", \"stabil\"]."]},{"cell_type":"code","execution_count":null,"id":"b89adaf1","metadata":{"id":"b89adaf1","executionInfo":{"status":"aborted","timestamp":1729107307708,"user_tz":-420,"elapsed":5,"user":{"displayName":"21_177 PRIO BUDI LAKSONO","userId":"05194090561344682492"}}},"outputs":[],"source":["def tokenize(text):\n","    tokens = text.split()\n","    return tokens\n","\n","df['tokenize'] = df['case_folding'].apply(tokenize)\n","\n","df.head(5)"]},{"cell_type":"markdown","id":"1f4f0960","metadata":{"id":"1f4f0960"},"source":["### STOPWORD REMOVAL\n","\n","Stopword removal adalah proses menghapus kata-kata yang dianggap tidak penting atau tidak memiliki makna signifikan dalam analisis teks, seperti \"dan,\" \"di,\" \"yang,\" atau \"itu.\" Kata-kata ini sering muncul dalam kalimat tetapi tidak memberikan informasi penting untuk pemrosesan atau analisis lebih lanjut. Dengan menghapus stopwords, data teks menjadi lebih ringkas dan fokus hanya pada kata-kata yang memiliki bobot lebih besar dalam analisis, seperti saat melakukan klasifikasi atau pemodelan teks."]},{"cell_type":"code","execution_count":null,"id":"5161fae2","metadata":{"id":"5161fae2","executionInfo":{"status":"aborted","timestamp":1729107307708,"user_tz":-420,"elapsed":5,"user":{"displayName":"21_177 PRIO BUDI LAKSONO","userId":"05194090561344682492"}}},"outputs":[],"source":["from nltk.corpus import stopwords\n","nltk.download('stopwords')\n","stop_words = stopwords.words('indonesian')"]},{"cell_type":"code","execution_count":null,"id":"aaff8355","metadata":{"id":"aaff8355","executionInfo":{"status":"aborted","timestamp":1729107307708,"user_tz":-420,"elapsed":4,"user":{"displayName":"21_177 PRIO BUDI LAKSONO","userId":"05194090561344682492"}}},"outputs":[],"source":["def remove_stopwords(text):\n","  return [word for word in text if word not in stop_words]\n","\n","df['stopword_removal'] = df['tokenize'].apply(lambda x: ' '.join(remove_stopwords(x)))\n","\n","\n","df.to_csv(\"preprocessing-cnnnews.csv\", encoding='utf8', index=False)\n","df.head(5)"]},{"cell_type":"markdown","id":"651c9282","metadata":{"id":"651c9282"},"source":["## **TF-IDF (Term Frequency-Inverse Document Frequency)**\n","\n","TF-IDF adalah metode statistik yang digunakan untuk mengevaluasi pentingnya suatu kata dalam sebuah dokumen relatif terhadap koleksi dokumen lainnya. TF-IDF sering digunakan dalam tugas seperti penggalian teks, penambangan informasi, dan pemodelan pembelajaran mesin berbasis teks.\n","Term Frequency (TF), yang menghitung seberapa sering sebuah kata muncul dalam dokumen, dan Inverse Document Frequency (IDF), yang menilai seberapa jarang kata tersebut muncul di seluruh dokumen dalam koleksi.\n","\n","TF-IDF bekerja dengan memberikan bobot lebih tinggi pada kata-kata yang sering muncul dalam sebuah dokumen, tetapi jarang muncul di dokumen lain, sehingga membantu mengidentifikasi kata-kata yang paling relevan.\n"]},{"cell_type":"code","execution_count":null,"id":"7ad19d5c","metadata":{"id":"7ad19d5c","executionInfo":{"status":"aborted","timestamp":1729107307708,"user_tz":-420,"elapsed":4,"user":{"displayName":"21_177 PRIO BUDI LAKSONO","userId":"05194090561344682492"}}},"outputs":[],"source":["import pandas as pd\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","\n","df = pd.read_csv(\"preprocessing-cnnnews.csv\")\n","\n","# Menginisialisasi TfidfVectorizer\n","vectorizer = TfidfVectorizer()\n","\n","# Menghitung TF-IDF\n","tfidf_matrix = vectorizer.fit_transform(df['stopword_removal'])"]},{"cell_type":"code","execution_count":null,"id":"4a39f251","metadata":{"id":"4a39f251","executionInfo":{"status":"aborted","timestamp":1729107307708,"user_tz":-420,"elapsed":4,"user":{"displayName":"21_177 PRIO BUDI LAKSONO","userId":"05194090561344682492"}}},"outputs":[],"source":["# Mengubah hasilnya menjadi DataFrame\n","tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=vectorizer.get_feature_names_out())\n","tfidf_df.head(10)"]},{"cell_type":"code","execution_count":null,"id":"c580dbda","metadata":{"id":"c580dbda","executionInfo":{"status":"aborted","timestamp":1729107307708,"user_tz":-420,"elapsed":4,"user":{"displayName":"21_177 PRIO BUDI LAKSONO","userId":"05194090561344682492"}}},"outputs":[],"source":["# Menginisialisasi TfidfVectorizer dengan normalisasi L2\n","vectorizer = TfidfVectorizer(norm='l2')\n","\n","# Menghitung TF-IDF dengan normalisasi L2\n","tfidf_matrix = vectorizer.fit_transform(df['stopword_removal'])\n","\n","# Mengubah hasilnya menjadi DataFrame\n","tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=vectorizer.get_feature_names_out())\n","\n","# Menampilkan 10 baris pertama\n","tfidf_df.head(10)\n"]},{"cell_type":"code","execution_count":null,"id":"f1c88775","metadata":{"id":"f1c88775","executionInfo":{"status":"aborted","timestamp":1729107307708,"user_tz":-420,"elapsed":4,"user":{"displayName":"21_177 PRIO BUDI LAKSONO","userId":"05194090561344682492"}}},"outputs":[],"source":["print(df.columns)\n"]},{"cell_type":"code","execution_count":null,"id":"5445838f","metadata":{"id":"5445838f","executionInfo":{"status":"aborted","timestamp":1729107307708,"user_tz":-420,"elapsed":4,"user":{"displayName":"21_177 PRIO BUDI LAKSONO","userId":"05194090561344682492"}}},"outputs":[],"source":["import pandas as pd\n","from sklearn.model_selection import train_test_split\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.metrics import classification_report, confusion_matrix\n","\n","# Asumsikan Anda memiliki DataFrame 'df' dengan kolom 'stopword_removal' dan 'kategori'\n","# Buat fitur (X) dan label (y)\n","X = tfidf_df\n","y = df['kategori']  # Pastikan 'kategori' adalah nama kolom yang benar\n","\n","# Pisahkan dataset menjadi set pelatihan dan pengujian (80% pelatihan, 20% pengujian)\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","\n","# Inisialisasi model Regresi Logistik\n","model = LogisticRegression(max_iter=1000)\n","\n","# Latih model\n","model.fit(X_train, y_train)\n","\n","# Lakukan prediksi pada set pengujian\n","y_pred = model.predict(X_test)\n","\n","# Evaluasi model\n","print(\"Confusion Matrix:\")\n","print(confusion_matrix(y_test, y_pred))\n","print(\"\\nHasil Klasifikasi:\")\n","print(classification_report(y_test, y_pred, zero_division=0))\n","\n","\n","# Opsional, simpan model untuk digunakan nanti\n","import joblib\n","joblib.dump(model, 'model_regresi_logistik.pkl')\n"]}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.6"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":5}